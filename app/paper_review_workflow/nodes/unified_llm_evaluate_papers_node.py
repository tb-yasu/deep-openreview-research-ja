"""Unified LLM evaluation node - 1å›ã®å‘¼ã³å‡ºã—ã§å…¨è©•ä¾¡ã‚’å®Œçµ."""

import json
import re
from typing import Any

from langchain_openai import ChatOpenAI
from loguru import logger

from app.paper_review_workflow.models.state import (
    PaperReviewAgentState,
    EvaluatedPaper,
)
from app.paper_review_workflow.config import (
    LLMConfig,
    ScoringWeights,
    DEFAULT_SCORING_WEIGHTS,
)
from app.paper_review_workflow.constants import (
    MIN_SCORE,
    MAX_SCORE,
    MAX_AUTHORS_DISPLAY,
    MAX_KEYWORDS_DISPLAY,
)


class UnifiedLLMEvaluatePapersNode:
    """çµ±åˆLLMè©•ä¾¡ãƒãƒ¼ãƒ‰ - ã‚¿ã‚¤ãƒˆãƒ«ã€ã‚¢ãƒ–ã‚¹ãƒˆã€ãƒ¬ãƒ“ãƒ¥ãƒ¼å…¨ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ä½¿ã£ã¦1å›ã§å…¨è©•ä¾¡."""
    
    def __init__(
        self,
        llm_config: LLMConfig | None = None,
        scoring_weights: ScoringWeights | None = None,
    ) -> None:
        """UnifiedLLMEvaluatePapersNodeã‚’åˆæœŸåŒ–.
        
        Args:
        ----
            llm_config: LLMè¨­å®šï¼ˆçœç•¥æ™‚ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
            scoring_weights: ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°é‡ã¿è¨­å®šï¼ˆçœç•¥æ™‚ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
        """
        from app.paper_review_workflow.config import DEFAULT_LLM_CONFIG
        
        self.llm_config = llm_config or DEFAULT_LLM_CONFIG
        self.weights = scoring_weights or DEFAULT_SCORING_WEIGHTS
        self.llm = self._create_llm()
    
    def _create_llm(self):
        """LLMã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ."""
        model_name = self.llm_config.model.value
        
        if model_name.startswith("gpt"):
            return ChatOpenAI(
                model=model_name,
                temperature=self.llm_config.temperature,
                max_tokens=self.llm_config.max_tokens,
                timeout=self.llm_config.timeout,
            )
        else:
            raise ValueError(f"Unsupported model: {model_name}. Only OpenAI GPT models are supported.")
    
    def __call__(self, state: PaperReviewAgentState) -> dict[str, Any]:
        """çµ±åˆLLMè©•ä¾¡ã‚’å®Ÿè¡Œ.
        
        Args:
        ----
            state: ç¾åœ¨ã®çŠ¶æ…‹
            
        Returns:
        -------
            æ›´æ–°ã•ã‚ŒãŸçŠ¶æ…‹ã®è¾æ›¸
        """
        logger.info(f"ğŸ¤– Unified LLM evaluation for {len(state.ranked_papers)} papers using {self.llm_config.model.value}...")
        logger.info(f"ğŸ“Š 1å›ã®å‘¼ã³å‡ºã—ã§å…¨ã‚¹ã‚³ã‚¢ + ãƒ¬ãƒ“ãƒ¥ãƒ¼è¦ç´„ + field_insights ã‚’å–å¾—")
        
        evaluated_papers: list[EvaluatedPaper] = []
        
        for i, paper in enumerate(state.ranked_papers, 1):
            try:
                logger.info(f"  [{i}/{len(state.ranked_papers)}] Evaluating: {paper.title[:50]}...")
                
                # çµ±åˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ
                prompt = self._create_unified_evaluation_prompt(paper, state.evaluation_criteria)
                
                # LLMã«è©•ä¾¡ã‚’ä¾é ¼ï¼ˆ1å›ã®å‘¼ã³å‡ºã—ï¼‰
                response = self.llm.invoke(prompt)
                response_text = response.content
                
                # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ‘ãƒ¼ã‚¹
                evaluation = self._parse_llm_response(response_text)
                
                # è«–æ–‡ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’æ›´æ–°
                updated_paper = paper.model_copy(deep=True)
                updated_paper.relevance_score = evaluation['relevance']
                updated_paper.novelty_score = evaluation['novelty']
                updated_paper.impact_score = evaluation['impact']
                updated_paper.practicality_score = evaluation['practicality']
                updated_paper.review_summary = evaluation['review_summary']
                updated_paper.field_insights = evaluation['field_insights']
                updated_paper.ai_rationale = evaluation['rationale']
                
                # overall_scoreã‚’è¨ˆç®—ï¼ˆ4ã¤ã®ã‚¹ã‚³ã‚¢ã®é‡ã¿ä»˜ãå¹³å‡ï¼‰
                updated_paper.overall_score = (
                    evaluation['relevance'] * 0.4 +
                    evaluation['novelty'] * 0.25 +
                    evaluation['impact'] * 0.25 +
                    evaluation['practicality'] * 0.10
                )
                
                evaluated_papers.append(updated_paper)
                
                logger.debug(
                    f"    âœ“ Scores: R={evaluation['relevance']:.2f} "
                    f"N={evaluation['novelty']:.2f} "
                    f"I={evaluation['impact']:.2f} "
                    f"P={evaluation['practicality']:.2f} "
                    f"Overall={updated_paper.overall_score:.2f}"
                )
                
            except Exception as e:
                logger.warning(f"  âš  Failed to evaluate paper {paper.id}: {e}")
                # è©•ä¾¡å¤±æ•—æ™‚ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®š
                updated_paper = paper.model_copy(deep=True)
                updated_paper.relevance_score = 0.5
                updated_paper.novelty_score = 0.5
                updated_paper.impact_score = 0.5
                updated_paper.practicality_score = 0.5
                updated_paper.overall_score = 0.5
                updated_paper.review_summary = "è©•ä¾¡ã«å¤±æ•—ã—ã¾ã—ãŸ"
                updated_paper.field_insights = "N/A"
                updated_paper.ai_rationale = f"LLMè©•ä¾¡ã‚¨ãƒ©ãƒ¼: {str(e)[:100]}"
                evaluated_papers.append(updated_paper)
                continue
        
        logger.success(f"âœ… Successfully evaluated {len(evaluated_papers)} papers with unified LLM")
        
        return {
            "llm_evaluated_papers": evaluated_papers,
        }
    
    def _create_unified_evaluation_prompt(self, paper: EvaluatedPaper, criteria) -> str:
        """çµ±åˆè©•ä¾¡ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ - 1å›ã®å‘¼ã³å‡ºã—ã§å…¨ã¦å®Œçµ."""
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç ”ç©¶èˆˆå‘³
        research_interests_str = ", ".join(criteria.research_interests)
        user_interests = criteria.research_description or f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {research_interests_str}"
        
        # ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        reviews_formatted = self._format_dynamic_reviews(paper.reviews)
        
        prompt = f"""
ã‚ãªãŸã¯æ©Ÿæ¢°å­¦ç¿’è«–æ–‡ã®è©•ä¾¡å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®è«–æ–‡ã‚’ç·åˆçš„ã«è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚

# ğŸ“„ è«–æ–‡æƒ…å ±

**ã‚¿ã‚¤ãƒˆãƒ«**: {paper.title}

**è‘—è€…**: {', '.join(paper.authors[:MAX_AUTHORS_DISPLAY])}{'...' if len(paper.authors) > MAX_AUTHORS_DISPLAY else ''}

**ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰**: {', '.join(paper.keywords[:MAX_KEYWORDS_DISPLAY])}

**ã‚¢ãƒ–ã‚¹ãƒˆãƒ©ã‚¯ãƒˆ**:
{paper.abstract[:1500]}{'...' if len(paper.abstract) > 1500 else ''}

**æ¡æŠåˆ¤å®š**: {paper.decision or 'N/A'}

**æ¡æŠåˆ¤å®šã‚³ãƒ¡ãƒ³ãƒˆ** (Program Chairs):
{(paper.decision_comment[:500] + '...') if paper.decision_comment and len(paper.decision_comment) > 500 else (paper.decision_comment or 'N/A')}

# ğŸ“Š OpenReview ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿

{reviews_formatted}

# ğŸ¯ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç ”ç©¶èˆˆå‘³

{user_interests}

# ğŸ“ è©•ä¾¡ã‚¿ã‚¹ã‚¯

ä»¥ä¸‹ã®**4ã¤ã®ã‚¹ã‚³ã‚¢**ã‚’0.0-1.0ã®ç¯„å›²ã§è©•ä¾¡ã—ã¦ãã ã•ã„ï¼š

## 1. é–¢é€£æ€§ (relevance)
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç ”ç©¶èˆˆå‘³ã¨ã®é–¢é€£åº¦ã‚’è©•ä¾¡ã€‚
- è«–æ–‡ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€ã‚¿ã‚¤ãƒˆãƒ«ã€ã‚¢ãƒ–ã‚¹ãƒˆãƒ©ã‚¯ãƒˆã‹ã‚‰åˆ¤æ–­
- ãƒ¬ãƒ“ãƒ¥ãƒ¼ã« "relevance" ã‚„ "significance" ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒã‚ã‚Œã°å‚è€ƒã«ã™ã‚‹

## 2. æ–°è¦æ€§ (novelty)
ç ”ç©¶ã®ç‹¬å‰µæ€§ãƒ»æ–°ã—ã•ã‚’è©•ä¾¡ã€‚
- ãƒ¬ãƒ“ãƒ¥ãƒ¼ã® **"originality"** ã‚„ **"novelty"** ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒã‚ã‚Œã°å„ªå…ˆçš„ã«ä½¿ç”¨
- **"strengths_and_weaknesses"** ã«æ–°è¦æ€§ã®è¨˜è¿°ãŒã‚ã‚Œã°å‚è€ƒ
- **"claims_and_evidence"** ã‚„ **"contribution"** ã‚‚å‚è€ƒ
- ãªã‘ã‚Œã°ã‚¢ãƒ–ã‚¹ãƒˆãƒ©ã‚¯ãƒˆã‹ã‚‰æ¨æ¸¬

## 3. ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆ (impact)
å­¦è¡“çš„ãƒ»å®Ÿç”¨çš„ãªå½±éŸ¿åŠ›ã‚’è©•ä¾¡ã€‚
- ãƒ¬ãƒ“ãƒ¥ãƒ¼ã® **"significance"** ã‚„ **"contribution"** ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒã‚ã‚Œã°å„ªå…ˆ
- **"rating"** ã‚„ **"overall_recommendation"** ã‚‚é‡è¦–
- æ¡æŠåˆ¤å®š (Accept/Reject) ã‚‚è€ƒæ…®
- **"experimental_designs_or_analyses"** ã®è³ªã‚‚å‚è€ƒ

## 4. å®Ÿç”¨æ€§ (practicality)
å®Ÿéš›ã®å¿œç”¨å¯èƒ½æ€§ã‚’è©•ä¾¡ã€‚
- å®Ÿè£…ã®å®¹æ˜“æ€§ã€å†ç¾æ€§ã€ç”£æ¥­å¿œç”¨ã®å¯èƒ½æ€§
- **"methods_and_evaluation_criteria"** ã‚„ **"code_of_conduct"** ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚‚å‚è€ƒ
- ãƒ¬ãƒ“ãƒ¥ãƒ¼ã® **"questions_for_authors"** ã‚‚å‚è€ƒ

## 5. ãƒ¬ãƒ“ãƒ¥ãƒ¼è¦ç´„ (review_summary)
ã™ã¹ã¦ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’çµ±åˆã—ã¦ã€2-3æ–‡ã§è¦ç´„ã—ã¦ãã ã•ã„ï¼š
- ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ¯ãƒ¼ã®ä¸»ãªè©•ä¾¡ç‚¹ï¼ˆå¼·ã¿ãƒ»å¼±ã¿ï¼‰
- å¹³å‡çš„ãªè©•ä¾¡å‚¾å‘
- Program Chairsã®åˆ¤å®šç†ç”±ï¼ˆã‚ã‚Œã°ï¼‰

## 6. ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰æ´»ç”¨ã®èª¬æ˜ (field_insights)
ã©ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ä¸»ã«ä½¿ç”¨ã—ãŸã‹ã‚’1-2æ–‡ã§èª¬æ˜ï¼š
ä¾‹: "ICMLã®overall_recommendationãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰(å¹³å‡3.0)ã¨summaryã‚’ä¸»ã«å‚ç…§ã—ã¾ã—ãŸ"
ä¾‹: "NeurIPSã®ratingãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰(å¹³å‡5.5)ã¨strengths_and_weaknessesã‚’ä¸»ã«å‚ç…§ã—ã¾ã—ãŸ"

# å‡ºåŠ›å½¢å¼

å¿…ãšä»¥ä¸‹ã®JSONå½¢å¼ã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼ˆèª¬æ˜æ–‡ã¯ä¸è¦ï¼‰ï¼š

{{
  "relevance": 0.85,
  "novelty": 0.72,
  "impact": 0.68,
  "practicality": 0.80,
  "review_summary": "ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ¯ãƒ¼ã¯æ‰‹æ³•ã®ç†è«–çš„å …ç‰¢æ€§ã‚’é«˜ãè©•ä¾¡ã€‚ä¸€æ–¹ã§å®Ÿé¨“ã®é™å®šæ€§ã‚’æŒ‡æ‘˜ã€‚Program Chairsã¯æ–°è¦æ€§ã¨å®Ÿé¨“å“è³ªã®ãƒãƒ©ãƒ³ã‚¹ã‹ã‚‰æ¡æŠã‚’æ¨å¥¨ã€‚",
  "field_insights": "ICMLã®overall_recommendation(å¹³å‡2.75)ã€theoretical_claimsã€experimental_designs_or_analysesãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ä¸»ã«å‚ç…§ã—ã¾ã—ãŸã€‚",
  "rationale": "ã“ã®è«–æ–‡ã¯ã‚°ãƒ©ãƒ•ç”Ÿæˆã«ç‰¹åŒ–ã—ã¦ãŠã‚Šã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®èˆˆå‘³ã«ç›´æ¥é–¢é€£ã€‚æ–°ã—ã„æ‰‹æ³•ã§å®Ÿé¨“ã‚‚å……å®Ÿã—ã¦ã„ã‚‹ãŒã€å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®æ¤œè¨¼ãŒé™å®šçš„ã€‚"
}}
"""
        return prompt
    
    def _format_dynamic_reviews(self, reviews: list[dict]) -> str:
        """å‹•çš„ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’å«ã‚€ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’èª­ã¿ã‚„ã™ããƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ."""
        if not reviews:
            return "**ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿ãªã—**ï¼ˆæ¡æŠæ¸ˆã¿ã ãŒãƒ¬ãƒ“ãƒ¥ãƒ¼ãŒéå…¬é–‹ã€ã¾ãŸã¯å–å¾—ã‚¨ãƒ©ãƒ¼ï¼‰"
        
        formatted_lines = []
        
        for i, review in enumerate(reviews, 1):
            formatted_lines.append(f"## ãƒ¬ãƒ“ãƒ¥ãƒ¼ {i}")
            formatted_lines.append("")
            
            # é‡è¦ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’å„ªå…ˆè¡¨ç¤º
            priority_fields = {
                'rating': 'ã‚¹ã‚³ã‚¢',
                'overall_recommendation': 'ã‚¹ã‚³ã‚¢', 
                'confidence': 'ç¢ºä¿¡åº¦',
                'summary': 'è¦ç´„',
            }
            
            for field, label in priority_fields.items():
                if field in review:
                    value = review[field]
                    # é•·ã™ãã‚‹å ´åˆã¯çœç•¥
                    display = value[:300] + "..." if len(value) > 300 else value
                    formatted_lines.append(f"**{label}**: {display}")
            
            formatted_lines.append("")
            
            # ãã®ä»–ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼ˆã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆé †ã€æœ€å¤§10å€‹ã¾ã§ï¼‰
            other_fields = {k: v for k, v in review.items() 
                           if k not in priority_fields}
            
            if other_fields:
                formatted_lines.append("**ãã®ä»–ã®è©•ä¾¡é …ç›®**:")
                for j, (field, value) in enumerate(sorted(other_fields.items()), 1):
                    if j > 10:  # é•·ã™ãã‚‹å ´åˆã¯çœç•¥
                        formatted_lines.append(f"  ...ä»– {len(other_fields) - 10} é …ç›®")
                        break
                    # ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åã‚’èª­ã¿ã‚„ã™ã
                    field_display = field.replace('_', ' ').title()
                    # å€¤ã‚’çœç•¥
                    display = value[:150] + "..." if len(value) > 150 else value
                    formatted_lines.append(f"  â€¢ **{field_display}**: {display}")
            
            formatted_lines.append("")
            formatted_lines.append("---")
            formatted_lines.append("")
        
        return "\n".join(formatted_lines)
    
    def _parse_llm_response(self, response: str) -> dict:
        """LLMã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦è©•ä¾¡çµæœã‚’æŠ½å‡º."""
        try:
            # JSONãƒ–ãƒ­ãƒƒã‚¯ã‚’æŠ½å‡º
            json_match = re.search(r'```json\s*(\{.*?\})\s*```', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                # JSONãƒ–ãƒ­ãƒƒã‚¯ãŒãªã„å ´åˆã€å…¨ä½“ã‹ã‚‰{}ã‚’æ¢ã™
                json_match = re.search(r'\{.*?\}', response, re.DOTALL)
                if json_match:
                    json_str = json_match.group(0)
                else:
                    # å…¨ä½“ã‚’JSONã¨ã—ã¦ãƒ‘ãƒ¼ã‚¹
                    json_str = response.strip()
            
            # JSONã‚’ãƒ‘ãƒ¼ã‚¹
            evaluation = json.loads(json_str)
            
            # ã‚¹ã‚³ã‚¢ã‚’0-1ã®ç¯„å›²ã«ã‚¯ãƒªãƒƒãƒ—
            return {
                'relevance': max(MIN_SCORE, min(MAX_SCORE, float(evaluation.get('relevance', 0.5)))),
                'novelty': max(MIN_SCORE, min(MAX_SCORE, float(evaluation.get('novelty', 0.5)))),
                'impact': max(MIN_SCORE, min(MAX_SCORE, float(evaluation.get('impact', 0.5)))),
                'practicality': max(MIN_SCORE, min(MAX_SCORE, float(evaluation.get('practicality', 0.5)))),
                'review_summary': str(evaluation.get('review_summary', 'ãƒ¬ãƒ“ãƒ¥ãƒ¼è¦ç´„ãªã—'))[:500],
                'field_insights': str(evaluation.get('field_insights', 'ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰æƒ…å ±ãªã—'))[:300],
                'rationale': str(evaluation.get('rationale', 'è©•ä¾¡ç†ç”±ãªã—'))[:500],
            }
        except Exception as e:
            logger.warning(f"Failed to parse LLM response: {e}")
            logger.debug(f"Response: {response[:300]}...")
            # ãƒ‘ãƒ¼ã‚¹å¤±æ•—æ™‚ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            return {
                'relevance': 0.5,
                'novelty': 0.5,
                'impact': 0.5,
                'practicality': 0.5,
                'review_summary': 'LLMè©•ä¾¡ã®ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ã¾ã—ãŸ',
                'field_insights': 'ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼',
                'rationale': f'ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {str(e)[:100]}',
            }

