"""Node for ranking evaluated papers."""

import re
from typing import Any

from langchain_openai import ChatOpenAI
from loguru import logger

from app.paper_review_workflow.models.state import (
    PaperReviewAgentState,
    EvaluatedPaper,
    EvaluationCriteria,
)
from app.paper_review_workflow.utils import convert_papers_to_dict_list
from app.paper_review_workflow.constants import (
    MAX_DISPLAY_PAPERS,
    PRELIMINARY_LLM_MAX_TOKENS,
    ABSTRACT_SHORT_LENGTH,
    MAX_KEYWORDS_DISPLAY,
)


class RankPapersNode:
    """è©•ä¾¡æ¸ˆã¿è«–æ–‡ã‚’ã‚¹ã‚³ã‚¢é †ã«ãƒ©ãƒ³ã‚¯ä»˜ã‘ã™ã‚‹ãƒãƒ¼ãƒ‰."""
    
    def __init__(self) -> None:
        """RankPapersNodeã‚’åˆæœŸåŒ–."""
        self.llm = None  # å¿…è¦æ™‚ã«åˆæœŸåŒ–ï¼ˆã‚³ã‚¹ãƒˆå‰Šæ¸›ï¼‰
    
    def __call__(self, state: PaperReviewAgentState) -> dict[str, Any]:
        """è«–æ–‡ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’å®Ÿè¡Œ.
        
        Args:
        ----
            state: ç¾åœ¨ã®çŠ¶æ…‹
            
        Returns:
        -------
            æ›´æ–°ã•ã‚ŒãŸçŠ¶æ…‹ã®è¾æ›¸
        """
        logger.info(f"Ranking {len(state.evaluated_papers)} evaluated papers...")
        
        # è©•ä¾¡åŸºæº–ã«åŸºã¥ã„ã¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        criteria = state.evaluation_criteria
        filtered_papers = [
            paper for paper in state.evaluated_papers
            if self._meets_criteria(paper, criteria)
        ]
        
        logger.info(
            f"After filtering: {len(filtered_papers)}/{len(state.evaluated_papers)} papers "
            f"meet the criteria"
        )
        
        # ç·åˆã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆï¼ˆé™é †ï¼‰
        ranked_papers = sorted(
            filtered_papers,
            key=lambda p: p.overall_score or 0.0,
            reverse=True,
        )
        
        # ç°¡æ˜“LLMãƒ•ã‚£ãƒ«ã‚¿ï¼ˆæœ‰åŠ¹ãªå ´åˆï¼‰
        if criteria.enable_preliminary_llm_filter and len(ranked_papers) > 0:
            logger.info("ğŸ” Preliminary LLM filter enabled - evaluating top candidates...")
            ranked_papers = self._apply_preliminary_llm_filter(
                ranked_papers, 
                criteria
            )
        
        # top_kãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã€ä¸Šä½kä»¶ã®ã¿é¸æŠ
        if criteria.top_k_papers is not None:
            selected_papers = ranked_papers[:criteria.top_k_papers]
            logger.info(
                f"Selected top {criteria.top_k_papers} papers from {len(ranked_papers)} ranked papers "
                f"(actual: {len(selected_papers)})"
            )
        else:
            selected_papers = ranked_papers
            logger.info(f"All {len(ranked_papers)} papers selected (no top_k limit)")
        
        # ä¸Šä½è«–æ–‡ã‚’è¾æ›¸å½¢å¼ã«å¤‰æ›ï¼ˆè¡¨ç¤ºç”¨ï¼‰
        top_papers = convert_papers_to_dict_list(
            selected_papers,
            max_count=MAX_DISPLAY_PAPERS,
            include_llm_scores=False,
        )
        
        if top_papers:
            logger.success(f"Top paper: {top_papers[0]['title'][:50]} (Score: {top_papers[0]['overall_score']:.3f})")
        
        return {
            "ranked_papers": selected_papers,  # LLMè©•ä¾¡ã«æ¸¡ã™è«–æ–‡ãƒªã‚¹ãƒˆ
            "top_papers": top_papers,
        }
    
    def _meets_criteria(self, paper: EvaluatedPaper, criteria: EvaluationCriteria) -> bool:
        """è«–æ–‡ãŒè©•ä¾¡åŸºæº–ã‚’æº€ãŸã™ã‹ãƒã‚§ãƒƒã‚¯.
        
        Args:
        ----
            paper: è©•ä¾¡æ¸ˆã¿è«–æ–‡
            criteria: è©•ä¾¡åŸºæº–
            
        Returns:
        -------
            åŸºæº–ã‚’æº€ãŸã™å ´åˆTrue
        """
        # é–¢é€£æ€§ã‚¹ã‚³ã‚¢ã®æœ€å°å€¤ãƒã‚§ãƒƒã‚¯
        if paper.relevance_score is not None:
            if paper.relevance_score < criteria.min_relevance_score:
                return False
        
        # ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚¹ã‚³ã‚¢ã®æœ€å°å€¤ãƒã‚§ãƒƒã‚¯
        if criteria.min_rating is not None and paper.rating_avg is not None:
            if paper.rating_avg < criteria.min_rating:
                return False
        
        return True
    
    def _apply_preliminary_llm_filter(
        self, 
        ranked_papers: list[EvaluatedPaper], 
        criteria: EvaluationCriteria,
    ) -> list[EvaluatedPaper]:
        """ç°¡æ˜“LLMè©•ä¾¡ã§relevance_scoreã‚’å†è¨ˆç®—ã—ã€å†ã‚½ãƒ¼ãƒˆ.
        
        Args:
        ----
            ranked_papers: ã‚½ãƒ¼ãƒˆæ¸ˆã¿è«–æ–‡ãƒªã‚¹ãƒˆ
            criteria: è©•ä¾¡åŸºæº–
            
        Returns:
        -------
            relevance_scoreã‚’æ›´æ–°ã—ã¦å†ã‚½ãƒ¼ãƒˆã—ãŸè«–æ–‡ãƒªã‚¹ãƒˆ
        """
        # è©•ä¾¡å¯¾è±¡æ•°ã‚’æ±ºå®š
        filter_count = min(
            criteria.preliminary_llm_filter_count,
            len(ranked_papers)
        )
        
        logger.info(f"Evaluating top {filter_count} papers with LLM for better relevance scoring...")
        
        # LLMåˆæœŸåŒ–
        if self.llm is None:
            self.llm = ChatOpenAI(
                model="gpt-4o-mini",
                temperature=0.0,
                max_tokens=PRELIMINARY_LLM_MAX_TOKENS,
            )
        
        # ä¸Šä½Nä»¶ã‚’ç°¡æ˜“LLMè©•ä¾¡
        updated_papers = []
        success_count = 0
        
        for i, paper in enumerate(ranked_papers[:filter_count], 1):
            try:
                # LLMã§é–¢é€£æ€§ã‚’è©•ä¾¡
                llm_relevance = self._evaluate_relevance_with_llm(paper, criteria)
                
                # relevance_scoreã‚’æ›´æ–°
                updated_paper = paper.model_copy(deep=True)
                old_score = paper.relevance_score or 0.0
                updated_paper.relevance_score = llm_relevance
                
                # overall_scoreã‚‚æ›´æ–°ï¼ˆrelevance_weightã‚’è€ƒæ…®ï¼‰
                # overall_score = relevance * weight + novelty * weight + impact * weight
                # ç°¡æ˜“çš„ã«relevanceã®å·®åˆ†ã‚’åæ˜ 
                score_diff = llm_relevance - old_score
                updated_paper.overall_score = (paper.overall_score or 0.0) + score_diff * 0.4  # relevance_weight=0.4
                
                updated_papers.append(updated_paper)
                success_count += 1
                
                if i % 50 == 0:
                    logger.info(f"  Progress: {i}/{filter_count} papers evaluated")
                
            except Exception as e:
                logger.warning(f"Failed to LLM evaluate paper {paper.id}: {e}")
                # å¤±æ•—æ™‚ã¯å…ƒã®ã‚¹ã‚³ã‚¢ã‚’ä¿æŒ
                updated_papers.append(paper)
        
        # æ®‹ã‚Šã®è«–æ–‡ï¼ˆLLMè©•ä¾¡ã—ãªã„ï¼‰ã‚’è¿½åŠ 
        remaining_papers = ranked_papers[filter_count:]
        all_papers = updated_papers + remaining_papers
        
        # relevance_scoreã§å†ã‚½ãƒ¼ãƒˆï¼ˆoverall_scoreã«åæ˜ ã•ã‚Œã¦ã„ã‚‹ã®ã§ã€overall_scoreã§ã‚½ãƒ¼ãƒˆï¼‰
        re_ranked_papers = sorted(
            all_papers,
            key=lambda p: p.overall_score or 0.0,
            reverse=True,
        )
        
        logger.success(
            f"âœ“ Preliminary LLM filter completed: {success_count}/{filter_count} papers re-scored"
        )
        
        return re_ranked_papers
    
    def _evaluate_relevance_with_llm(
        self, 
        paper: EvaluatedPaper, 
        criteria: EvaluationCriteria,
    ) -> float:
        """LLMã§è«–æ–‡ã®é–¢é€£æ€§ã‚’ç°¡æ˜“è©•ä¾¡.
        
        Args:
        ----
            paper: è©•ä¾¡å¯¾è±¡è«–æ–‡
            criteria: è©•ä¾¡åŸºæº–
            
        Returns:
        -------
            é–¢é€£æ€§ã‚¹ã‚³ã‚¢ï¼ˆ0.0-1.0ï¼‰
        """
        # ã‚¢ãƒ–ã‚¹ãƒˆãƒ©ã‚¯ãƒˆã‚’çŸ­ç¸®
        abstract_short = (
            paper.abstract[:ABSTRACT_SHORT_LENGTH] + 
            ("..." if len(paper.abstract) > ABSTRACT_SHORT_LENGTH else "")
        )
        keywords_str = ", ".join(paper.keywords[:MAX_KEYWORDS_DISPLAY])
        
        # research_description ãŒãªã„å ´åˆã¯ research_interests ã‚’ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        research_interests_str = ", ".join(criteria.research_interests)
        user_interests = criteria.research_description or f"Keywords: {research_interests_str}"
        
        prompt = f"""Rate the relevance of this paper to the user's research interests.

User's Research Interests:
{user_interests}

Paper:
Title: {paper.title}
Keywords: {keywords_str}
Abstract: {abstract_short}

Rate the relevance on a scale of 0.0 to 1.0:
- 1.0: Highly relevant, directly addresses the research interests
- 0.7-0.9: Very relevant, closely related
- 0.4-0.6: Moderately relevant, some overlap
- 0.1-0.3: Slightly relevant, tangential connection
- 0.0: Not relevant

Return ONLY a single number between 0.0 and 1.0 (e.g., "0.85"). No other text.
"""
        
        try:
            response = self.llm.invoke(prompt)
            response_text = response.content.strip()
            
            # æ•°å€¤ã‚’æŠ½å‡º
            # "0.85"ã®ã‚ˆã†ãªå½¢å¼ã€ã¾ãŸã¯"The relevance is 0.85"ã®ã‚ˆã†ãªå½¢å¼ã«å¯¾å¿œ
            match = re.search(r'(\d+\.?\d*)', response_text)
            if match:
                score = float(match.group(1))
                # 0-1ã®ç¯„å›²ã«åˆ¶é™
                score = max(0.0, min(1.0, score))
                return score
            else:
                logger.warning(f"Could not parse LLM response: {response_text[:50]}")
                return paper.relevance_score or 0.5
                
        except Exception as e:
            logger.warning(f"LLM evaluation failed: {e}")
            return paper.relevance_score or 0.5

